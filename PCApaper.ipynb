{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual derivation of PCA using Spectra <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction.\" data-toc-modified-id=\"Introduction.-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction.</a></span><ul class=\"toc-item\"><li><span><a href=\"#Underlying-concepts-of-PCA\" data-toc-modified-id=\"Underlying-concepts-of-PCA-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Underlying concepts of PCA</a></span></li></ul></li><li><span><a href=\"#Methods\" data-toc-modified-id=\"Methods-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Methods</a></span><ul class=\"toc-item\"><li><span><a href=\"#Simulated-molecular-signals\" data-toc-modified-id=\"Simulated-molecular-signals-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Simulated molecular signals</a></span></li><li><span><a href=\"#Simulated-Complex-Mixtures\" data-toc-modified-id=\"Simulated-Complex-Mixtures-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Simulated Complex Mixtures</a></span></li><li><span><a href=\"#PCA\" data-toc-modified-id=\"PCA-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>PCA</a></span></li></ul></li><li><span><a href=\"#Underlying-maths-of-PCA\" data-toc-modified-id=\"Underlying-maths-of-PCA-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Underlying maths of PCA</a></span><ul class=\"toc-item\"><li><span><a href=\"#Main-Equation\" data-toc-modified-id=\"Main-Equation-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Main Equation</a></span></li><li><span><a href=\"#Scores\" data-toc-modified-id=\"Scores-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Scores</a></span></li><li><span><a href=\"#PCs-(Loadings)\" data-toc-modified-id=\"PCs-(Loadings)-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>PCs (Loadings)</a></span></li><li><span><a href=\"#PCs-as-Subtraction-Spectra\" data-toc-modified-id=\"PCs-as-Subtraction-Spectra-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>PCs as Subtraction Spectra</a></span></li></ul></li><li><span><a href=\"#Calculation-of-PCA\" data-toc-modified-id=\"Calculation-of-PCA-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Calculation of PCA</a></span><ul class=\"toc-item\"><li><span><a href=\"#Initialisation\" data-toc-modified-id=\"Initialisation-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Initialisation</a></span></li><li><span><a href=\"#Calculation\" data-toc-modified-id=\"Calculation-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Calculation</a></span></li></ul></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Summary</a></span></li><li><span><a href=\"#Acknowledgements\" data-toc-modified-id=\"Acknowledgements-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Acknowledgements</a></span></li><li><span><a href=\"#Conflicts-of-Interest\" data-toc-modified-id=\"Conflicts-of-Interest-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Conflicts of Interest</a></span></li><li><span><a href=\"#Funding\" data-toc-modified-id=\"Funding-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Funding</a></span></li><li><span><a href=\"#Appendices\" data-toc-modified-id=\"Appendices-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Appendices</a></span><ul class=\"toc-item\"><li><span><a href=\"#Splitting-positive-and-negative-contributions\" data-toc-modified-id=\"Splitting-positive-and-negative-contributions-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Splitting positive and negative contributions</a></span></li><li><span><a href=\"#Sanity-Checks\" data-toc-modified-id=\"Sanity-Checks-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>Sanity Checks</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T11:35:25.041236Z",
     "start_time": "2018-03-16T11:35:24.953580Z"
    }
   },
   "source": [
    "\n",
    "J Renwick Beattie$^1$ and Francis Esmond-White$^2$\n",
    "\n",
    "$^1$J Renwick Beattie Consulting, Ballycastle, Co. Antrim UK. ORCID ID:0000-0002-0205-717X\n",
    "\n",
    "$^2$\n",
    "\n",
    "Corresponding Author: J Renwick Beattie, rene@jrenwickbeattie.com\n",
    "\n",
    "# Introduction.\n",
    "Principal Component Analysis (PCA) is a method to simplify complex datasets by summarising them in terms of basic building blocks that are found to repeat over and over across the dataset. It organises these building block such that they explain non-overlapping information ordered by the extent to which they account for variation observed in the data. It assumes these building blocks each have unique characteristics and that each observation contains its different mixtures of the individual building blocks. The aim of PCA is to uncover the identity of these building blocks and use them to explain the data as succinctly as possible. Wold listed the goals of PCA as including: Simplification, Data Reduction, Modelling, Outlier Detection, Variable Selection, Classification, Prediction and Unmixing [S. Wold, K.H. Esbensen, P. Geladi, Principal components analysis, Chemom. Intell. Lab. Syst. 2 (1987) 37–52].\n",
    "\n",
    "\n",
    "PCA, attributed to Pearson in 1901 [K. Pearson, On lines and planes of closest fit to systems of points in space., Philos. Mag. 2 (1901) 559–572.], is a very popular mathematical tool in spectroscopic fields, with 16,600 papers for the 5 years between 2015 and 2019 (inclusive) found searching the term ‘Spectroscopy “principal component analysis” OR \"PCA\"’ in Google Scholar [https://scholar.google.com/scholar? hl=en&as_sdt=1%2C5&as_ylo=2015&as_yhi=2019&as_vis=1&q=%E2%80%98 Spectroscopy+%E2%80%9Cprincipal+component+analysi s%E2%80%9D+OR+%22PCA%22%E2%80%99&btnG=  accessed 23rd August 2019], out of 32,900 results returned when leaving out the spectroscopy term. The main attraction of PCA and other data reduction methods for spectroscopic study is straightforward, they take multiple variables and filter them down into a much smaller and more manageable number of variables while retaining most of the information the original variables contained.  It also needs no information outside of the spectral data to perform the calculation, making it an excellent tool for discovery and exploratory analysis.\n",
    "\n",
    "Spectra often contain hundreds or thousands of variables, which presents a number of problems, including:\n",
    " - sanity checking datasets\n",
    " - comprehension of the dataset\n",
    " -  high false positive rates with multiple hypothesis testing (family-wise error rate and per family error rate [H.J. Keselman, Per Family or Familywise Type I Error Control: “Eether, Eyether, Neether, Nyther, Let’s Call the Whole Thing Off!,” J. Mod. Appl. Stat. Methods. 14 (2015) 5–6. doi:10.22237/jmasm/1430453100]).\n",
    " - Overfitting\n",
    " \n",
    "Modelling based on spectral variables is effectively multiple parallel hypothesis testing on thousands of variables. This leads to overfitting as the false positive rate is going to be high, especially for classification problems where the number of states is very limited. Overfitting occurs where a model is fitted that includes features unrelated to the variation of interest (i.e. 'noise' in a generic sense).\n",
    "\n",
    "Data reduction approaches such as PCA condense the information into a much more usable set of variables, allowing comprehensive review of the underlying processes much more succinctly. In many digital signals, and especially spectra, a high level of redundancy is present in the data since analytes display patterns of non-point features (i.e. bands wider than a pixel) inducing a high variable-wise correlation between variables. Data reduction methods exploit the correlation present to retain information while reducing the variables needed to the basis set of correlations that explain the variation in the dataset. Fewer variables subject to hypothesis testing lowers the risk of false positives.\n",
    "\n",
    "Because PCA reduces variables it simplifies the amount of variables to be examined, leading to new options for interpretation of the dataset. All PCs are orthogonal (no *mathematical* information in a PC is present in any other) and they provide a very comprehensive summary of the whole dataset. There is, however, a limit to interpretability. Because it uses only the dataset’s internal covariance (correlation is covariance ratioed to the overall variance) structure it means that any analytes that exhibit correlated behaviour (e.g. fat soluble components vs water soluble components) will be mixed in the PCs. To interpret further, back to specific individual analytes it is necessary to deconvolve the PCs or to employ alternative data reduction methods such as Ordinary Least Squares (OLS) or modified versions of PCA using different bounds. OLS is where the data is projected onto a set of expected analytes, contaminants and matrix components. This allows the user to build readily interpretable models. The problem with OLS and other modelling methods is that they require recording the analytes separately and as such may not capture all the spectral perturbations that arise (see in-vivo perturbation of $\\alpha$-tocopherol in Beattie et al. [J.R. Beattie, C. Maguire, S. Gilchrist, L.J. Barrett, C.E. Cross, F. Possmayer, M. Ennis, J.S. Elborn, W.J. Curry, J.J. McGarvey, B.C. Schock, The use of Raman microscopy to determine and localize vitamin E in biological samples., FASEB J. 21 (2007) 766–76. doi:10.1096/fj.06-7028com.]) and also needs the user to anticipate the range of analytes etc that they need to record. Other data reduction methods can place restrictions on the loadings such as non-negativity (Multivariate Curve Resolution, MCR) in order to force the calculation of more ‘natural’ loadings without negative features. Factor Analysis can also impose and additional orthogonality constraint on the scores as well as the loadings. \n",
    "\n",
    "Many papers have been written over the years explaining PCA [T. Davies, T. Fearn, Back to basics: the principles of principal component analysis, Spectrosc. Eur. 16 (2004) 20.\n",
    ";\tA.M.C. Davies, T. Fearn, Back to basics : applications of principal component analysis, Spectrosc. Eur. 17 (2005) 30–31.\n",
    "; A.M.C. Davies, T. Fearn, Back to basics: when you need more than Principal Component Analysis, Spectrosc. Eur. 17 (2005) 37–39.\n",
    "); S. Wold, K.H. Esbensen, P. Geladi, Principal components analysis, Chemom. Intell. Lab. Syst. 2 (1987) 37–52.; ], with a strong emphasis on the mathematical nature of the method. This article is intended to attempt to bridge the gap between a rigorous mathematical exploration and a typical spectroscopy scientist’s intuition. To achieve this we will explore the process of calculating PCA visually, observing the changes on the data associated with each operation within the calculation. It is aimed at primarily at applied spectroscopists or other digital signal analysts who understand the fundamental meaning of those wiggly lines and who prefer to work with spectral visualizations over mathematical formulas. The version of calculating PCA presented is optimized for understanding rather than computation speed and therefore will not be the most efficient algorithm available. The code is freely available to support the reader in undertaking their own explorations. It is focused on the calculation and meaning of basic PCA and in order to retain a tight focus on matters most pertinent to this goal will not be touching on a wide range of relevant ancillary matters such as scaling, outlier detection, model performance metrics, cross validation etc. \n",
    "\n",
    "In order to keep the discussion generalised every effort is made to avoid application specific interpretation, even to the exclusion of typical x and y units (these matter for interpretation, not for performing PCA). To eliminate the influence of spectral errors from consideration the paper uses a simulated dataset. These simulations are based on a body of published work on using Raman spectra for the analysis of fatty acids. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underlying concepts of PCA\n",
    "\n",
    "The building blocks of a dataset, alluded to above, are unknown to PCA and the algorithm seeks through the data to find them. The unknown-ness of these building blocks is why they are often referred to as ‘latent variables’, where latent means something that is present but not directly observed, i.e. hidden. To do this the data is treated like a series of simultaneous equations, with each row of data treated as a unique function:\n",
    "$$Obs_1 = A_1*x + B_1*y + C_1*z \\ldots\\\\Obs_2 = A_2*x + B_2*y + C_2*z \\ldots\\;\\;\\;\\;\\label{eqObs1}$$ \n",
    "\n",
    "Where $Obs_i$ is the $i$-*th* observation row, $[A_i,B_i,C_i]$ is the coefficient for each underlying variable and $[x,y,z]$ are the underlying variables, each of which are vectors comprised of hundreds or thousands of elements.\n",
    "\n",
    "Because these observations contain hundreds or thousands of variables it appears daunting to solve, but PCA exploits a neat trick. This trick depends on the fact that variables interact with each other and that these variables do not represent isolated independent information. They share information and so they have a correlation that reflects the amount of information they share. Items that correlate (covary) together are grouped into what is known as a Principal Component (PC, also eigenvector, loading) each describing one set of correlated behaviours from the dataset. This means that all the variables contributing to one piece of information are captured in one place. \n",
    "\n",
    "In statistical analysis it is highly desirable to use input variables that are as independent as possible so that there is no waste of effort collecting data that is providing the same information. In spectral analysis it is unavoidable that the input variables are interdependent because they are in fact related to each other according to the physical phenomena that underlie the spectroscopy.  A way of identifying new variables that minimize cross-talk is essential to allow cleaner and simpler statistical analysis. Each PC is orthogonal to each other, which means that they share no common information. In graphical terms they are different axes set at 90$^o$ to each other. This means that their statistical properties are suitable for a wide range of statistical analyses.\n",
    "\n",
    "\n",
    "Many spectra have hundreds or thousands of variables and checking the statistical significance of each one can become time consuming and, more importantly, increase the risk of a type II error in which a 'statistically significant' result is in fact based on random chance. If you are working to a significance level of 5% and exploit 1000 variables with random distribution relative to the parameter of interest the chances of finding a spurious relationship are very high, in fact for a infinite dataset around 50 correlations that arise due to chance alone would be expected to meet the criteria for statistical significance. In the real world such 'long-run' experiments have to be compromised by available resources and the situation is much worse. In a study powered to fairly common statistical practices such as powering using an alpha of 5% and beta 80%, the false positive rate will be much higher, around 25% [Ioannidis JPA (2005) Why Most Published Research Findings Are False. PLoS Med 2(8): e124. https://doi.org/10.1371/journal.pmed.0020124], or in other words 250 variables will appear significant (although signals are correlated, the shot noise in each channel is assumed independent). By first combining redundant information across the variables through the process of data reduction you can reduce the number of statistical tests carried out and therefore the risk of a type II error.\n",
    "\n",
    "One way of looking at correlations is that they summarise the amount of redundant information shared between two variables and so methods that can coalesce correlated variables would be able to reduce redundancy of information. PCA and related data reduction methods analyse a dataset to uncover what patterns of correlation occur between variables that can explain the variations observed. Because it reduces the data based on covariance/correlation, the resulting information is orthogonal, that is the variation described by each PC is uncorrelated to others. This means that the PCs returned summarise the patterns in variation much more succinctly than the original spectra, allowing much simpler statistical modelling. For this reason data reduction including PCA is used as an initial step prior to a very wide range of analyses such as classification, neural networks and decision trees. PCA can dramatically simplify the complexity of a spectral dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods\n",
    "\n",
    "In order to create a dataset with full traceability simulation was chosen. This allows complete control over the variation and factors influencing the data fluctuations. The aim was to have a dataset where the variation observed was due to variation in the underlying molecular/chemical composition of the sample and avoiding as many confounding sources of variation and noise as possible. This allows a more uncluttered understanding of what is happening during the PCA analysis. The code and simulated spectra are provided to allow users to add in any additional complications they may wish in order to observe how the PCA is then perturbed.\n",
    "\n",
    "## Simulated molecular signals\n",
    "\n",
    "The first stage of the simulation process was to create a simulated pure Raman spectra of a series of individual fatty acid species that are prevalent in butterfat [^1] 4:0; 6:0; 8:0; 10:0; 12:0; 14:0; 14:1c[Delta]7; 15:0; 16:0; 16:1t[Delta]8; 16:1c[Delta]8; 17:0; 18:0; 18:1t[Delta]9; 18:1c[Delta]9; 18:2t[Delta]9,12; 18:2c[Delta]9,12; 18:3c[Delta]9,12,15 and conjugated linoleic acid (at time of original referenced manuscript the methodology was not capable of discriminating specific isomers). This was performed in MATLAB (v2013b, Cambridge, MA). The positions, intensities, widths and Gaussian/Lorentzian proportion of the 11 most intense peaks (ca. 1750, 1670, 1655, 1460, 1440, 1300, 1260, 1120, 1080, 1060 and 1020 cm$^{-1}$) were predicted from best-fit trends of the main peaks discussed in [J.R. Beattie, S.E.J. Bell, B.W. Moss, A critical evaluation of Raman spectroscopy for the analysis of lipids: Fatty acid methyl esters, Lipids. 39 (2004) 407–419. doi:10.1007/s11745-004-1245-z]. The predictions of the best fit ensured that the variation in peak properties are smooth between the fatty acids, simplifying the intensity changes present in the data. One band of the fatty acid spectra was simulated at constant intensity (equivalent to the carbonyl mode in the Raman spectra of fatty acid esters, which is present in the proportion of one carbonyl per fatty acid chain so acts as a molar standard. The individual peaks were then simulated  using Gaussian and Lorentzians and mixed together using the Gaussian/Lorentzian proportion. The individual peaks were then summed to give the final compound signal for each PC.\n",
    "\n",
    "The final simulated fatty acid signals are provided as input into the supplied code.\n",
    "\n",
    "\n",
    "[^1]: shorthand notation for fatty acids is number of carbons in chain:number of unsaturated bonds/isomer/[Delta]position so oleic acidm with 18 carbons, one unsaturated bond between carbon 9 and 10 (counting from the carbonyl moeity) is 18:9c[Delta]9\n",
    "\n",
    "## Simulated Complex Mixtures\n",
    "\n",
    "The relative contribution of each fatty acid to the sample signals is based on gas chromatography analysis in [J.R. Beattie, S.E.J. Bell, C. Borgaard, A.M. Fearon, B.W. Moss, Multivariate prediction of clarified butter composition using Raman spectroscopy, Lipids. 39 (2004) 897–906. doi:10.1007/s11745-004-1312-56]. The simulated individual fatty acid spectra were simply scaled by their relative molar contribution to the fatty acid profile then the sum of the individual spectra gave the final simulated sample spectrum. \n",
    "\n",
    "In order to ensure that the variability of the dataset was as minimal and clear as possible a number of simplifications were used to minimise the number of competing artefacts that could contribute to the dataset. No shot- nor instrumental-noise sources were simulated, no baselines distortions were added, no sampling variation, no intensity variation and no interactions between the individual molecular entities were simulated.\n",
    "\n",
    "## PCA\n",
    "\n",
    "The PCA was performed by a custom NIPALs algorithm [Henning Risvik Principal Component Analysis (PCA) & NIPALS algorithm] implemented in Python code [will need to reference this appropriately when accepted, depends on how journal handles this].  This code is implemented for academic exploration rather than efficiency, for a more efficient implementation of NIPALS use the available functionality in pypi. For verification a PCA was carried out using the sklearn package and code comparing the outputs is included in the supplied python code, and the outputs are provided as supplementary information in the appendix.\n",
    "\n",
    "Except where explicitly stated the data was mean-centred to follow general practice in spectroscopy. No further pre-processing was applied to avoid distractions from core focus on the derivation of PCA.\n",
    "\n",
    "This manuscript was produced in Jupyter notebook, requiring the files graphicalPCA.py, nipals.py and the following packages: pypandoc, numpy, scipy, matplotlib, sklearn, os and pdb. Analysis of the simulated data and production of figures can be replicated by running the following code in Jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T16:25:15.866529Z",
     "start_time": "2019-12-06T16:24:22.218166Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "from src import graphicalPCA as g_PCA\n",
    "from src.file_locations import images_folder\n",
    "\n",
    "gPCA = g_PCA.graphicalPCA()\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Readers are encouraged to load their own data and/or manipulate variables and/or settings within the graphicalPCA.py file to explore any aspect of the process further. If the user is using their own data then simplified simulations are recommended if the central aim is understanding the PCA process in more familiar spectra, as this allows complete understanding of the factors that influence the variation the final dataset.\n",
    "\n",
    "# Underlying maths of PCA\n",
    "\n",
    "## Main Equation\n",
    "\n",
    "The basic equation describing PCA is very simple\n",
    "\n",
    "$$ D=SL^\\top\\;\\;\\;\\;\\label{eqDSLT}$$\t\t\t\n",
    "\n",
    "$D$ is the \\underline{D}ata matrix of $[n,m]$ elements with \\underline{o}bservations $o$ by row and \\underline{v}ariables $v$ by column. This means that the data matrix contains $n$ rows of spectra, each containing $m$ spectral shifts aligned by column.\n",
    "\n",
    "$S$ is a matrix of observation-wise weightings or \\underline{S}cores, with n rows of scores for each observtion $o$. The maximum dimensionality (or number of underlying latent \\underline{f}actors, $f$) is determined by the smallest dimension of $D$ as this places the limit on how many simultaneous equations can be derived. If $n\\geq m$ then $S$ is $n x m$, otherwise $S$ is $n x n$. The \\underline{p}rincipal components are denoted $p$, with the maximum value of $p=f$.\n",
    "\n",
    "$L$ is a matrix of variable-wise weightings commonly referred to as \\underline{L}oadings. The weightings are standardised against the vector norm to give unit vectors. Unit vectors have the useful property that the sum of its squared intensities (vector norm) is equal to one. This means that upon matrix multiplication, in common with the use of 1 in arithmetic, this matrix gives the same result whether it is multiplied or inverted. In this equation it is denoted by convention as $L^\\top$ because the loading matrix that is returned by calculation of the PCA algorthim is a matrix whose rows corresponds to a spectral shift, whereas each column corresponds to a spectral shift in the $D$ matrix, i.e. it needs to be transposed to align ($^\\top$). \n",
    "\n",
    "This equation is very simple, only three terms, with just matrix multiplication on one side. There really is nothing complicated about this equation and we can simply rearrange equation 1 above to see for ourselves what each term means.\n",
    "\n",
    "The visual representation of equation \\eqref{eqDSLT} is in Figure \\ref{DSLT}.\n",
    "\n",
    "![Main PCA Equation Figure: The basis of PCA is that data can be represented as comprised of building blocks that are composed of variables correlated in their behaviour (i.e. act as a uniform entity). A) The dataset $D$ can be equivalently represented by (is equal to) a set of basic building blocks C) ($L^\\top$) and the magnitude of each building block within each sample B) ($S$). Data are offset for clarity with a dashed line at the local zero. Matrix dimensions and their orientation are indicated on the label. \\label{DSLT}](./docs/img/Graphical%20PCA%20Demo%20DSLT%20Eqn%20faded.png)\n",
    "\n",
    "## Scores\n",
    "\n",
    " To solve for $S$ we can pre-multiply by the inverse of $L^\\top$, or we can exploit the presence of unit vectors and multiply by $L^\\top$ itself. For most datasets (when $n≠m$) it is not possible to truly invert the matrices and so the pseudo-inverse is calculated which is a least squares approximation of inversion, and so not as precise as multiplication (although generally this is many orders of magnitude below the precision attainable in scientific measurements so is often not a concern in applied sciences).\n",
    "\n",
    "$$ S=L^\\dagger D\\;\\;=L^\\top D\\;\\; ≃L^+D \\;\\;\\;\\;\\label{eqSDL}$$ \n",
    "\n",
    "which is visually represented in Figure \\ref{SDL}\n",
    "\n",
    "![Figure \\ref{SDL}  Score Equation Figure: Principal Component scores (A) are calculated by linear algebraic multiplication of the principal components (B and C) themselves by the dataset.  Data are offset for clarity with a dashed line at the local zero. \\label{SDL}](./docs/img/Graphical%20PCA%20Demo%20SDL%20Eqn%20faded.png)\n",
    "\n",
    "This result shows that the $S$ matrix is the multiplication of the  $L^\\top$ matrix by the data matrix. If we unfold this into the individual score for observation $o$ in the $p^{th}$ principal component (Note: $v$ denotes variable number within the arrays, while $l_p$ and $d_o$ are one dimensional vectors extracted from the parent matrices $L$ and $D$):\n",
    "$$ s_{(o,p)}= \\sum_{v=1}^{v=m} l_{p}^\\top*d_{o}\\;\\;\\;\\;\\label{eqsld}$$\n",
    "\n",
    "Which expands to:\n",
    "\n",
    "$$ s_{(o,p)} = l_{(1,p)}^\\top*d_{(o,1)} + l_{(2,p)}^\\top*d_{(o,2)} + \\ldots l_{(m,p)}^\\top*d_{(o,m)} \\;\\;\\;\\;\\label{eqsld2}$$\n",
    "\n",
    "or visually as Figure \\ref{sldiL}:\n",
    "\n",
    "![Figure \\ref{sldiL}  *p*$^{th}$ PC Score Equation Figure: the equivalent array operation for an individual sample (A) is multiplying each variable in the sample signal by the same variable weighting in the component (B) and then summing the resulting vector to give a scalar (C). \\label{sldiL}](./docs/img/Graphical%20PCA%20Demo%20sldi%20Eqn.png)\n",
    "\n",
    "where $p$ is the index for the principal component, $o$ is the index for the sample and $v$ is the index for the spectral variable. Each PC score for every sample is the sum of the multiplication of the sample spectrum by the loading for that PC. Where peak intensity in the sample coincides with peak intensity in the loading the two multiply to create a value with a large magnitude. Where the peak intensity and/or the loading has low intensity the product will be of negligible magnitude. As a result, the score for a spectrum will depend on how well the peak intensity of the spectrum matches that of the loading. This means that the score an indicator of how much of a given PC is present in that sample spectrum.\n",
    "\n",
    "If we use the scores to scale the loadings then subtract the result from the data (this gives us the residual unexplained spectral intensity after accounting for the current PCs) we can gain some insight into the consequences of each PC. In Figure \\ref{sldiR} we can see that the scores with the largest magnitudes are reduced much more dramatically than the sample with a score close to zero. This is expected because samples close to zero have very low correlation with that PC, so the PC will have little impact on that sample. \n",
    "\n",
    "It is also interesting to note that before accounting for PC2 the extreme score spectra have a high negative correlation with each other, while after correction they have no similarities in their residual features. Also note that the residuals of the zero score spectrum and the extreme negative score spectrum happen to be strongly correlated ($R^2=0.93$), suggesting that a lower order PC will group these two samples close together. \n",
    "\n",
    "![Figure \\ref{sldiR}  *p*$^{th}$ PC Residual Figure: comparing the sample wise residuals after subtraction of the score scaled loading from the selected data. A and B are plotted on the same scale for direct comparison. \\label{sldiR}](./docs/img/Graphical%20PCA%20Demo%20sldiRes%20Eqn.png)\n",
    "\n",
    "\n",
    "## PCs (Loadings)\n",
    "\n",
    "Eqn 1 can also be rearranged by pre-multiplying each side by the inverse (or more usually the pseudo-inverse for non-invertible matrices) of the scores matrix\n",
    "\n",
    "$$ L^\\top=S^\\dagger D \\;\\;\\;\\;\\label{eqLTSiD}$$\n",
    "\n",
    "![Figure \\ref{LTSiD}  Component Equation Figure: The principal components themselves (A) are simply the score (B) weighted dataset (C) i.e. multiplied using linear algebra matrix operations \\label{LTSiD}](./docs/img/Graphical%20PCA%20Demo%20LTSiD%20Eqn%20faded.png)\n",
    "\n",
    "$S^\\dagger$ indicates the pseudo-inverse of the scores matrix. The scores matrix is not composed of unit vectors so cannot be pre-multiplied to give the same effect. Expanding this equation for the one dimensional $p^{th}$ PC (in spectra the ordering of the variables is systematic and so evaluating the PCs as an array is useful) we multiplty the one dimensional scores vector for the $p^{th}$ PC by the data matrix to get:\n",
    "\n",
    "$$ l_p= \\sum_{o=1}^{o=n}s_p^\\dagger*D \\;\\;\\;\\;\\label{eqlsdi}$$\n",
    "\n",
    "Which expands to:\n",
    "\n",
    "$$ l_p=s_{(1,p)}^\\dagger*d_1 + s_{(2,p)}^\\dagger*d_2+\\ldots s_{(n,p)}^\\dagger*d_n \\;\\;\\;\\;\\label{eqlsdix}$$\n",
    "\n",
    "or visually as Figure \\ref{lsdi}:\n",
    "\n",
    "![Figure \\ref{lsdi} *p*$^{th}$ Principal Component loading Equation Figure: the equivalent array operation for an individual component is multiplying each data signal by its score then summing the resulting matrix along the sample dimension. A) pseudo-inverse sample score. The 95% confidence intervals for the PC's scores are indicated along with the zero position B) Sample spectra. For ease of visual comparison the spectra have been scaled by the square root of the *p*$^{th}$ PC's eigenvalue and offset by the sample's score. B) the product of the score and data (no offset). C) the resulting sum of the products, which is the loading. The dashed line under each spectrum is its offset zero-line.\\label{lsdi}](./docs/img/Graphical%20PCA%20Demo%20lsdi%20Eqn.png)\n",
    "\n",
    "The loading for each PC is the pseudoinverse of the score times the sample spectrum then summed, i.e. each loading is the sum of the original spectra weighted by the inverse of the scores. Superficial examination of the loadings leads to the intuition that these loadings are in fact subtraction spectra. The author has discussed this with many esteemed colleagues and the most common response has been that PCs are mystic unknowable entity that can't be pinned down that easily.  \n",
    "\n",
    "## PCs as Subtraction Spectra\n",
    "\n",
    "Superficially the PC loadings, with some features appearing above the zero line and some below, may seem like subtraction spectra. If we consider the positive scored samples and the negative scored ones separately how do they relate to the PC? \n",
    "We can split the above equation into the positive score and negative score contributions as in \\ref{eqlpni}:\n",
    "\n",
    "$$ l_p=\\sum_{o=1}^{o=n}s_{o}^{\\dagger}*d_{o}[s_o \\gt 0]\\; -\\;\\sum_{o=1}^{o=n}-s_{o}^{\\dagger}*d_{o}[s_o \\lt 0] \\;\\;\\;\\;\\label{eqlpni}$$\n",
    "\n",
    "where [ ] are Iverson brackets indicating that inclusion of an observation $o$ in the sum is dependent on the condition within the square brackets being true. In essence what is being done in \\eqref{eqlpni} is that the spectra giving a positive score are weighted and summed separately from those with a negative score. The negative score magnitude is used so that it is clear the final step required to recreate the principal component is a subtraction. \n",
    "\n",
    "In order to present results that are easily interpreted we will briefly switch to discussing non-centered data so that the input data for the PCA are positive spectra, rather than already difference spectra. Figure  \\ref{lpni} shows the positive (yellow) and negative scores (blue) obtained in PC 2 and the individual spectra giving these scores coloured in the same way. Multiplying the positive score spectra by their scores gives the yellow summed spectrum in the middle column, while the magnitude of the negative scores multiplied by the spectra give the blue summed spectrum. The final column shows the original PC in magenta overlaid by the unit vector corresponding to the subtraction of the summed score weighted spectra in cyan. The difference between these two vectors is plotted in black and the mean magnitude of the difference is shown on the figure. As expected the two PC and the difference between the weighted spectra is within machine precision of zero.\n",
    "\n",
    "![Figure \\ref{lpni} PC2 +/- Score Equation Figure: each component can be considered as a score magnitude weighted subtraction signal of the sum of positive score signals minus the sum of negative score signals.\\label{lpni}](./docs/img/Graphical%20PCA%20Demo%20positive%20negative%20contributions%20PC_2.png)\n",
    "\n",
    "The mathematical derivation and the empirical analysis of the simulated data agree on the conclusion that the PCs are indeed subtraction spectra, where the sample spectra have been weighted by the scores associated with that PC. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculation of PCA\n",
    "\n",
    "It could be argued that one cannot be said to have fully understood something until they can produce it from scratch themselves. So, faced with a bare dataset of spectra acquired from a range of samples, how do you produce $S$ and $L^\\top$?\n",
    "\n",
    "Obviously one answer is to use code available in your preferred programming language, but this is not calculating from scratch. How do you get to the answer using just the data and basic mathematic (linear algebra) operations?  \n",
    "\n",
    "It turns out that there is not a single way of finding the solution, a number of different algorithms are used such as NIPALS, Singular Value Decomposition (SVD) and Lower-Upper Partial Decomposition. All share the same common aim of minimising residual variance as succinctly as possible. The procedure presented here is the NIPALS algorithm, as outlined in [Principal Component Analysis (PCA) & NIPALS algorithm Henning Risvik]. This mathematical procedure can be followed using code available alongside this manuscript, which breaks the NIPALs algorithm into very small steps for ease of stepping through and understanding each step as opposed to computational efficiency.\n",
    "\n",
    "## Initialisation\n",
    "\n",
    "In order to achieve this, the first step is to calculate the variance within the dataset, which is the square of the deviation in the data. If no form of centring is applied to the data then the deviation will initially be from zero rather than from the mean, but if the data is mean centred prior to PCA this deviation will be from the mean spectrum. This issue of centring will be returned to below.\n",
    "The first step therefore is to calculate the variance which is identical to the sum of squares. In linear algebra this is achieved by pre-multiplying the data matrix $D$ by its transpose $D^\\top$. This operation is performed as follows:\n",
    "\n",
    "$$C=D^\\top D\\;\\;\\;\\;\\label{eqDTD} \\eqref{eqDTD}$$\n",
    "\n",
    "where C is the covariance matrix (which is identical to a $p$ scaled version the correlation matrix if the data has been scaled to unit variance), which is calculated by routine matrix multiplication as follows: \n",
    "\n",
    " \n",
    "$$\\begin{vmatrix}d_{1,1}*d_{1,1}+d_{2,1}*d_{2,1}\\ldots+d_{o,1}*d_{o,1} & \\cdots & d_{1,1}*d_{1,v}+d_{2,1}*d_{2,v}\\ldots +d_{o,1}*d_{o,v}\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "d_{1,v}*d_{1,1}+d_{2,v}*d_{2,1}\\ldots+d_{o,v}*d_{o,1} & \n",
    "\\cdots & \n",
    "d_{1,v}*d_{1,v}+d_{2,v}*d_{2,v}\\ldots+d_{o,v}*d_{o,v}\n",
    "\\end{vmatrix}\\;\\;\\;\\;\\label{eqdtd}$$\n",
    "\n",
    "\n",
    "Which can be further simplified to:\n",
    "\n",
    "$$\\begin{vmatrix}d_{1,1}^2+d_{2,1}^2\\ldots+d_{o,1}^2 & \\cdots & As\\,Eqn\\ref{eqdtd}\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "As\\,Eqn\\ref{eqdtd} & \\cdots & d_{1,v}^2+d_{2,v}^2\\ldots+d_{o,v}^2\n",
    "\\end{vmatrix}\\;\\;\\;\\;\\label{eqdtdX}$$\n",
    "\n",
    "\n",
    "All elements within the diagonal are comprised of sums of each element in the original matrix multiplied by itself, i.e. the sum of squares. The vector operations equivalent to this are shown in Figure \\ref{DTD}\n",
    "\n",
    "![Figure \\ref{DTD} Sum of Squares Equation Figure: the first step in calculating PCA is to establish the total variance in the dataset and the pattern of covariance, this is achieved by the inner product of the dataset, the diagonal (trace) of which is the sum of squares for the entire dataset. \\label{DTD}](./docs/img/Graphical%20PCA%20Demo%20DTD%20Eqn.png)\n",
    "\n",
    "## Calculation \n",
    "The cycle of the iterative process for NIPALS is illustrated in Figure \\ref{NIPALS} below. \n",
    "\n",
    "![Figure \\ref{NIPALS} Schematic of the NIPALS process. i is the index of the PC, j is the iteration index within each PC. The initial residual is the data itself (in this example mean centred first and is the residual from the 0th PC). The unit vector of the sum of squares of the i-th PC residual is used to provide an initial estimate of sample weights for the (i+1)th PC. These are iteratively used to update estimates for the PC until stopping criteria are reached. \\label{NIPALS}](./docs/img/Graphical%20PCA%20Demo%20DTDw%20Eqn.png)\n",
    "\n",
    "Figure \\ref{NIPALS}, is coded by first initialising the PC ($p$) and iteration ($i$) counters at 0 then setting the initial residual to equal the data (mean centred in the example provided).\n",
    "\n",
    "```\n",
    "Programme NIPALS:\n",
    "\n",
    "INPUTS \n",
    "max_i, maximum iterations on each PC\n",
    "tol, acceptable tolerance for difference between iteration i and i-1 estimates of the PCs\n",
    "max_pc, desired maximum number of PCs\n",
    "D, data matrix with observation in rows, variables in columns\n",
    "\n",
    "CODE\n",
    "calculate n and m, the number of observations and variables in the data matrix\n",
    "ensure max_p is achievable (minimum of o,v and max_p)\n",
    "initialise the residual_p as the input data (Figure Xa)\n",
    "\n",
    "for p = 0,1,... max_p\n",
    "    reset iteration counter, i, to 0 (Figure Xa->b)\n",
    "    l_p,i = norm of sqrt(sum(residual_p^2))  (Figure Xb)\n",
    "    itChange = |l_p,i|\n",
    "\n",
    "        for i = 1,2,... max_i, while itChange<tol\n",
    "            w_p,i = outer product( residual_p ,l_p,i )  (Figure Xc)\n",
    "            l_p,i = inner product( w'_p,i-1 , residual_p) (Figure Xd)\n",
    "            l_p,i = l_p,i / norm(l_p,i)\n",
    "            itChange = sum(|l_p,i - l_p,i-1|) (Figure Xe)\n",
    "        endfor\n",
    "    s_p = outer product( residual_p , l_p,i ) \n",
    "    l_p = l_p,i\n",
    "    residual_p+1 = residual_p - inner product( s_p, l_p ) (Figure Xf)\n",
    "```\n",
    "\n",
    "As it is the diagonal that is critical in the calculation of PCA the sum of squares of the data matrix can be calculated as an array operation, as is done in NIPALS. The disadvantage of the iterative NIPALs approach is that because each PC is calculated iteratively in turn the errors due to machine precision can accumulate. This is not a problem when the data can be reduced to a very low dimensionality, but if the underlying structure of the data is complex and so requires many PCs to describe it, additional steps may be required to re-orthogonalise the data [Andrecut, M. (2009). \"Parallel GPU Implementation of Iterative PCA Algorithms\". Journal of Computational Biology. 16 (11): 1593–1599. PMID 19772385. doi:10.1089/cmb.2008.0221.]. \n",
    "\n",
    "The sum of squares initialises our algorithm by providing a target for weighting the data against. In the coded example the unit vector of the sum of squares is projected onto the data to provide initial estimates of the required weighting for each data sample. Two key issues in any iterative optimisation routine is choosing the starting and stopping parameters, but detailed discussion of either is not within the scope of this work. The code associated with this manuscript uses the unit vector of the sum of the absolute intensity of the spectra for the initial PC guess, commonly an individual sample vector is chosen.\n",
    "\n",
    "Each iteration recalculates the weights that improve the fit against the sum of squares. The individual spectra are then weighted and summed then set to unit vectors.\n",
    "\n",
    "\n",
    " This loading vector is multiplied with the data to update the estimate of weightings (features in the data that overlap with strong features in the initial loading are boosted, those that are weak or overlap with near zero regions of the loading yield lower weightings). These new weighting estimates are used to create a new iteration of the loading. This is cycled until a predetermined cut off criteria has been met. In the attached code the criterion used to end iteration was that the reduction in error compared to the previous iteration was less than 0.1%, so quite a lax standard. Readers are directed to other resources for the issue of starting and stopping criteria. [http://onlinelibrary.wiley.com/doi/10.1002/cem.1180040111/full and look up Wold reference mentioned in abstract] … [may need to find some more]\n",
    "\n",
    "After each PC is calculated the information related to it is eliminated from the dataset through calculating the new residual and these residual intensities are used as input into the next iteration. Each subsequent PC follows the above sequence using the residual left from the previous PC. Because the information that relates to each PC is subtracted from the original data before calculation of the next PC, it is no longer available for further PCs, leading to the orthogonality between loadings that is the central feature of PCA.\n",
    "\n",
    "\n",
    "Within each iteration the weightings iteratively adjusted in order to have the sum of the weighted intensities match the sum of the squared intensities as closely as possible. To phrase it another way the aim of PCA is least squares fitting of the data by itself. The matrix operation to create the target for each $pc_p$ is the sum of squares of the preceeding residual $R_{p-1}^\\top R_{p-1}$. The aim is to minimise the sum of squares in the residual left after accounting for $pc_p$, i.e. $R_{p}^\\top R_{p}$. \n",
    "\n",
    "The difference between the residual after $pc_p$ and the residual before its calculation is \n",
    "\n",
    "$$ R_p = R_{p-1}-s_p\\cdot l_{p}^\\top $$\n",
    "substituting for the loadings we get:\n",
    "$$= R_{p-1}-s_p\\otimes s_p^{\\dagger}\\cdot R_{p-1} $$\n",
    "\n",
    "where the outer product $s_p\\otimes s_p^{\\dagger}$ is the autocorrelation of the weights and their inverse (which is simply the weights rescaled by the eigenvalue). Therefore, what the calculation of each PC is aiming to achieve is a reweighting where $w_p=s_p\\otimes s_p^{\\dagger}$ of the data such that :\n",
    "\n",
    "$$\\displaystyle \\operatorname*{min}\\bigl(\\Sigma(R_{p-1}-w_{p}\\times R_{p-1})^2\\bigr)\\;\\;\\;\\;\\label{eqMinR} \\eqref{eqMinR}$$\n",
    "\n",
    "It minimises the magnitude of the difference between the residual and the score weighted residual, which is equivalent to maximising the explanation of variance. Note that $s$ and $l$ are vectors, so a single lineshape is being applied to all the data, any deviance from this lineshape creates a mismatch that prevents full fitting of the data.\n",
    "\n",
    "\n",
    "If all spectra are identical the $w_p$ will be a vector of ones and $\\Sigma(R_{p-1}) = \\Sigma(R_{p-1}*w_{p})$. If they differ only in scale then $s_o$ will vary between each sample, but the sum of the weighted spectra will still match the sum of squares\n",
    "\n",
    "If the spectra have no commonality then $w_p$ will be non-zero for the pixels with the unique magnitudes between features/samples and zero for the rest, the PC will be directly equivalent to using just that subset of pixels. \n",
    "\n",
    "If the pixels have strongly correlated behaviour (and in signals this is expected) then $w_p$ will lie somewhere between a perfect square and the contribution of a single variable. \n",
    "\n",
    "Random noise theortically has zero correlation and so it would be expected that $w_p$ will be less than the contribution of a single variable. In finite sampling it is possible to observe false positives, but this risk reduces as the number of independent observations grows and signal quality of the dataset improves. \n",
    "\n",
    "# Summary\n",
    "\n",
    "PCA is a widely used method that has opened up many avenues of research that otherwise would not have been possible. The current manuscript lays out the underlying foundations of PCA in a way that is intuitively accessible to spectroscopists. It makes clear the deep and direct connection between the data and the principal components. One of the authors has previously published novel exploitations of PCA (PCA based background correction) that are only possible because of the intuitive mechanisms that have been formalized in this paper. Applied spectroscopists understanding the connection between their data and the analysis of that data may see more ways to tap into its power and also understand more deeply how to interpret the output.\n",
    "\n",
    "# Acknowledgements\n",
    "\n",
    "The authors would like to acknowledge the invaluable advice and feedback from Andrew Jirasek of the University of British Columbia, Canada and Nils Asfeth of Nofima, Norway.\n",
    "\n",
    "# Conflicts of Interest\n",
    "\n",
    "The Authors declare that there is no conflict of interest. \n",
    "\n",
    "# Funding\n",
    "The authors received no financial support for the research, authorship, and/or publication of this article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendices\n",
    "## Splitting positive and negative contributions\n",
    "\n",
    "An obvious conclusion from the insight that loadings are simply subtraction spectra would be to calculate the positive and negative score spectra separately then interpretation will be easier. There is a caveat however, the spectra recovered in this operation are not purely explained by the constituents giving the positive and negative features. Rather, if we plot a series of PCs reconstructed as separate positive and negative features what we observed is a tendency for the spectra to look more and more similar, as observed in Figure \\ref{lpniUnSub27}. Each PC is orthogonal, and so contains no information on the other PCs, so all the spectral intensity not contributing to the current PC simply appears in equal amounts in the positive and negative score spectra.\n",
    "\n",
    "This common signal is easily constructed through sparse reconstruction of the mean of the positive and negative score constituents using the $1^{st}$ to the $(i-1)^{th}$ PC. This is included in Figure \\ref{lpniUnSub27} as the green spectrum in each column. Its intensity lies between the positive and negative score spectra at every point. A quick glance reveals that as the PC rank increases the positive and negative constituents comprising the adjacently ranked PC appear more and more similar, suggesting the two constituents share more and more features\n",
    "\n",
    "![Figure \\ref{lpniUnSub27} PC$_p$ +/- Score Equation Figure: each component can be considered as a score magnitude weighted subtraction signal of the sum of positive score signals minus the sum of negative score signals.\\label{lpniUnSub27}](./docs/img/Graphical%20PCA%20Demo%20lpni%20common%20raw%20PC2%20to%207.png)\n",
    "\n",
    "Since PCs describe the major sources of variation in descending order, and because each PC is orthogonal to all others we can in fact identify the variations not associated with the current PC. The non-current PCs contain the information about the variance that is not different between the constituents. To phrase it another way we can reconstruct the shared signal by sparse PCA reconstruction of the higher ranked PCs. Once reconstructed it is then possible to correct for the common contribution by simply subtracting off this common signal. We can do a reasonable job of this in the simulated data because we constructed it from scratch and therefore know the global (for any given chemical contributor) minimum at each pixel and so reduce the extent of over-subtraction that would occur if we used a naive zero baseline. this has been done in Figure \\ref{lpniSub27} and now the differences in the constituents are more apparent.\n",
    "\n",
    "![Figure \\ref{lpniSub27} PC$_p$ +/- Score Equation Figure: each component can be considered as a score magnitude weighted subtraction signal of the sum of positive score signals minus the sum of negative score signals.\\label{lpniSub27}](./docs/img/Graphical%20PCA%20Demo%20lpni%20common%20corrected%20PC2%20to%207.png)\n",
    "\n",
    "Since each PC describes successively less variance, then some may suspect that the common variance would monotonically increase. However, this would be an unwarranted guess as PCA reduces data based on deviation of each pixel from the mean which is a weak guide for spectral subtraction, and very unreliable where peaks overlap. Indeed, when we plot the scaling factors used to correct for the common signal we find that indeed the trend is not monotonic, with a reduction in subtraction factor in PC7 compared with PC6. \n",
    "\n",
    "![Figure \\ref{lpniCommon} Scaling Factors used to correct for common signal contribution vs PC rank. The horizontal dashed line indicates where the scaling factor is less than for the previous PC. \\label{lpniCommon}](./docs/img/Graphical%20PCA%20Demo%20lpni%20common%20signal%20scaling%20factors%20PC2%20to%2013.png)\n",
    "\n",
    "\n",
    "\n",
    "It is noticeable from Figure \\ref{lpniCommon}  that there are subtle over-subtractions in some constituents, so the reader nay suspect that this is the cause of the dip, but manual adjustment of the scaling factor to avoid local dips in the baseline reveals that the dip for PC7 is in fact underestimated. This agrees with Figure \\ref{lpniSub27} in which the spectra show strong dips suggesting the scaling factor is too high. Readers are encouraged to explore this subtraction for themselves using the associated code in order to satisfy themselves of this point. In real world data, which have not been constructed de-novo we do not have the luxury of being able to so closely pin down the scaling factor for the positive and negative contributions. It is out of scope for this manuscript to further explore how source separation for the common and constituent signals \n",
    "may be achieved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Checks\n",
    "\n",
    "![Figure \\ref{PureFA} Plot of the simulated simplified  spectra representing the pure chemical entity's spectrum for each individual fatty acid.\\label{PureFA}](./docs/img/Graphical%20PCA%20Demo%20Pure%20Fatty%20Acid%20Simulated%20Spectra.png)\n",
    "\n",
    "A plot of the simulated pure spectra is shown here, with the carbonyl modes at around 1730-40 fixed at a maximum intensity of 100 and a fixed width but variable position. This mode represents a vibration that occurs in a moeity present precisely once per molecule and is therefore an excellent candidate for molar standardisation. In order to avoid interpretation complications due to scaling the pure spectra are generated already scaled and are simulated at that same scale. The largest variation is due to changes is unsaturation in peaks at around 1270 (cis only) and 1660 (cis) /1670 (trans) cm$^{-1}$, while the remaining peaks vary according to the number of carbons in the fatty acid chain.\n",
    "\n",
    "![Figure \\ref{SimData} Plot of the simulated observational data.\\label{SimData}](./docs/img/Graphical%20PCA%20Demo%20All%20Simulated%20Observations%20Data%20Plot.png)\n",
    "The simulated observational data reflects the typical behavior of real world butterfat, with the dominant fatty acids being long (16-20 Carbon) and low unsaturation (typically 0 or 1, low amounts of 2 or 3 olefinic bonds). \n",
    "\n",
    "![Figure \\ref{SVD_o_NIP} Plot overlaying the first PC calculated using the NIPALS algorithm and that calculated by the SKLearn SVD based PCA.\\label{SVD_o_NIP}](./docs/img/Graphical%20PCA%20Demo%20Local%20NIPALS%20overlaid%20SKLearn%20SVD%20based%20Principal%20Components.png)\n",
    "\n",
    "The first PC is very similar between the SVD and NIPALs algorithms\n",
    "\n",
    "![Figure \\ref{SVD_m_NIP} Plot of the difference between the 1st PC calculated using the NIPALS algorithm and that calculated by the SKLearn SVD based PCA.\\label{SVD_m_NIP}](./docs/img/Graphical%20PCA%20Demo%20Local%20NIPALS%20minus%20SKLearn%20SVD%20based%20Principal%20Components.png)\n",
    "The magnitude of the difference is orders of magnitude lower than the scale of the loadings themselves (compare the scales in Figures \\ref{SVD_o_NIP} and \\ref{SVD_m_NIP})\n",
    "\n",
    "\n",
    "![Figure \\ref{Tol_SVD_NIP} Plot of the log sum of the difference between the 1st PC calculated using the NIPALS algorithm and that calculated by the SKLearn SVD based PCA vs the tolerance used in the NIPALs algorithm.\\label{Tol_SVD_NIP}](./docs/img/Graphical%20PCA%20Demo%20Local%20NIPALS%20minus%20SKLearn%20SVD%20vs%20Tolerance.png)\n",
    "The Tolerance has a direct almost log-linear influence on the difference between NIPALs and SVD until the difference approaches machine precision levels.\n",
    "\n",
    "![Figure \\ref{Rank_SVD_NIP} Plot of the log sum of the difference between the PCs calculated using the NIPALS algorithm and that calculated by the SKLearn SVD based PCA vs the PC rank. The trend for two tolerances is shown for comparision. \\label{Rank_SVD_NIP}](./docs/img/Graphical%20PCA%20Demo%20Local%20NIPALS%20minus%20SKLearn%20SVD%20vs%20Rank.png)\n",
    "\n",
    "\n",
    "The difference between SVD and NIPALs increases as the PC rank increases, in line with the observation of [Andrecut, M. (2009). \"Parallel GPU Implementation of Iterative PCA Algorithms\". Journal of Computational Biology. 16 (11): 1593–1599. PMID 19772385. doi:10.1089/cmb.2008.0221.] discussed above.\n",
    "\n",
    "One note of caution is that because the lower down the ranking a PC occurs, the smaller its eigenvalue is, this leads to the risk that errors in the estimation of the PCs could lead to rank switching. Although PC rank is used as a common motif to indicate the identity of a PC it is no such thing. Any vector's identity is its shape and orientation. If we use correlation to evaluate the relatedness of the vector shapes between PCs, as shown in Figure \\ref{PCRankbySVD_PCRankbyNIP}  we can see that as we approach 20 PCs in this dataset the PC ranking becomes unstable and rank no longer accurately conveys PC identity between the two methods. If we seek for the closest match in terms of correlation this declines more gradually (Figure \\ref{PC_Corr vs_Rank})\n",
    "\n",
    "![Figure \\ref{PCRankbySVD_PCRankbyNIP} Scatter plot for the PC ranks determined by SVD and NIPALS (using correlation to determine appropriate equivalence).\\label{PCRankbySVD_PCRankbyNIP}](./docs/img/Graphical%20PCA%20Demo%20Local%20NIPALS%20Rank%20vs%20SKLearn%20SVD%20Rank.png)\n",
    "\n",
    "![Figure \\ref{PC_Corr vs_Rank} Plot of the correlations between PCs (optimum and as ranked by the parent algorithm_label{PC_Corr vs_Rank}](./docs/img/Graphical%20PCA%20Demo%20Correlation%20between%20SVD%20and%20NIPALs%20by%20closest%20identity%20and%20by%20rank.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": ".venv_pca",
   "language": "python",
   "name": ".venv_pca"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "latex_metadata": {
   "affiliation": "J Renwick Beattie Consulting, Ballycastle, UK",
   "author": "J. Renwick Beattie",
   "title": "Visual PCA of Spectra"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "660px",
    "left": "-1462.14px",
    "right": "20px",
    "top": "2329.86px",
    "width": "348px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
